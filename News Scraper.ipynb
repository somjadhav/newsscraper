{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def create_db():\n",
    "    conn = sqlite3.connect('project.db')\n",
    "    c=conn.cursor()\n",
    "    c.execute('''CREATE TABLE news \n",
    "              (link text)''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def add_link(link):\n",
    "    conn=sqlite3.connect('project.db')\n",
    "    c=conn.cursor()\n",
    "    entry=(link)\n",
    "    c.execute(\"INSERT INTO news VALUES (?)\", (entry,))    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def drop():\n",
    "    conn = sqlite3.connect('project.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''DROP TABLE news''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def delete_all():\n",
    "    conn=sqlite3.connect('project.db')\n",
    "    c=conn.cursor()\n",
    "    c.execute(\"DELETE FROM news\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def select_all():\n",
    "    conn=sqlite3.connect('project.db')\n",
    "    c=conn.cursor()\n",
    "    c.execute(\"SELECT * FROM news\")\n",
    "    links_list = c.fetchall()\n",
    "    conn.close()\n",
    "    return links_list\n",
    "\n",
    "def is_scraped(link):\n",
    "    links_list = select_all()\n",
    "    for site_link in links_list:\n",
    "        if link in site_link:\n",
    "            return True\n",
    "    return False\n",
    "        \n",
    "#create_db()\n",
    "#add_link(\"som.com\")\n",
    "#drop()\n",
    "#delete_all()\n",
    "#print(select_all())\n",
    "#print(is_scraped(\"som.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://theintercept.com/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "\n",
    "results = soup.find(id=\"root\")\n",
    "top_headline=results.find_all('a',class_=\"Homepage-HomepageTopStoriesHeroNode-link\")\n",
    "primary_headlines=results.find_all('a',class_=\"Homepage-HomepageTopStoriesPrimaryNode-container\")\n",
    "secondary_headlines=results.find_all('a',class_=\"Homepage-HomepageTopStoriesSecondaryNode-HomepageTopStoriesSecondaryPromo-container\")\n",
    "\n",
    "links_list=[]\n",
    "message=\"The Intercept:\\n\\n\"\n",
    "\n",
    "for headline in top_headline:\n",
    "    title=headline.find('h1',class_=\"elements-Heading-heading elements-Heading-dark Homepage-HomepageTopStoriesHeroNode-headingText\")\n",
    "    link=headline['href']\n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        \n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "    \n",
    "for headline in primary_headlines:\n",
    "    title_block= headline.find('div',class_=\"Homepage-HomepageTopStoriesPrimaryNode-textBlock\")\n",
    "    title=title_block.find('h3',class_=\"elements-Heading-heading elements-Heading-dark Homepage-HomepageTopStoriesPrimaryNode-heading\")\n",
    "    link = headline['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        \n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "\n",
    "    \n",
    "for headline in secondary_headlines:\n",
    "    title=headline.find('h4',class_=\"elements-Heading-heading elements-Heading-dark Homepage-HomepageTopStoriesSecondaryNode-HomepageTopStoriesSecondaryPromo-heading\")\n",
    "    link=headline['href']\n",
    "\n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        \n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "\n",
    "#politics headlines\n",
    "    \n",
    "url = \"https://theintercept.com/politics/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results = soup.find(id=\"root\")\n",
    "\n",
    "headlines=results.find_all('a',class_=\"data-SpecialPromoData-container\")\n",
    "\n",
    "for article in headlines:\n",
    "    title=article.find('h4',class_=\"elements-Heading-heading elements-Heading-dark data-SpecialPromoData-heading\")\n",
    "    link=article['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        \n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "        \n",
    "#national security headlines\n",
    "    \n",
    "url = \"https://theintercept.com/national-security/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results = soup.find(id=\"root\")\n",
    "\n",
    "headlines=results.find_all('a',class_=\"data-SpecialPromoData-container\")\n",
    "\n",
    "for article in headlines:\n",
    "    title=article.find('h4',class_=\"elements-Heading-heading elements-Heading-dark data-SpecialPromoData-heading\")\n",
    "    link=article['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        \n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "        \n",
    "#world headlines\n",
    "    \n",
    "url = \"https://theintercept.com/world/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results = soup.find(id=\"root\")\n",
    "\n",
    "headlines=results.find_all('a',class_=\"data-SpecialPromoData-container\")\n",
    "\n",
    "for article in headlines:\n",
    "    title=article.find('h4',class_=\"elements-Heading-heading elements-Heading-dark data-SpecialPromoData-heading\")\n",
    "    link=article['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        title_text=title.text\n",
    "        title_text=title_text.replace('The Coronavirus Crisis','')\n",
    "        title_text=title_text.replace('The War on Immigrants','')\n",
    "        title_text=title_text.replace('Voices','')\n",
    "        print(title_text)\n",
    "        print(\"Link: \"+link,end=\"\\n\\n\")\n",
    "\n",
    "        message+=title.text\n",
    "        message+=\"\\n\"\n",
    "        message+=\"Link: \"\n",
    "        message+=link\n",
    "        message+=\"\\n\\n\"\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://cepr.net/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results_pre=soup.find(id=\"viewport\")\n",
    "\n",
    "\n",
    "links_list=[]\n",
    "\n",
    "message += \"\\nCEPR:\\n\\n\"\n",
    "\n",
    "#main page\n",
    "results=results_pre.find('div',class_=\"home-latest-article-wrap\")\n",
    "results=results.find('div',class_=\"english\")\n",
    "headlines = results.find_all('div',class_=\"latest-art-post\")\n",
    "\n",
    "for headline in headlines:\n",
    "    block=headline.find('h2')\n",
    "    sub_block = block.find('a')\n",
    "    title = sub_block.text\n",
    "    link = sub_block['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        message += title\n",
    "        message += \"\\n\"\n",
    "        message += \"Link: \"\n",
    "        message += link\n",
    "        message += \"\\n\\n\"\n",
    "\n",
    "        print(title)\n",
    "        print(link,end=\"\\n\\n\")\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "\n",
    "    \n",
    "#beat the press\n",
    "results=results_pre.find('div',class_=\"home-blog-left-wrap-inner\")\n",
    "headline_main = results.find_all('div',class_=\"featured-blog-item-wrap\")\n",
    "headlines_other = results.find_all('div',class_=\"blog-item-wrap\")\n",
    "\n",
    "for headline in headline_main:\n",
    "    block=headline.find('h3')\n",
    "    sub_block = block.find('a')\n",
    "    title = sub_block.text\n",
    "    link = sub_block['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        message += title\n",
    "        message += \"\\n\"\n",
    "        message += \"Link: \"\n",
    "        message += link\n",
    "        message += \"\\n\\n\"\n",
    "\n",
    "        print(title)\n",
    "        print(link,end=\"\\n\\n\")\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n",
    "\n",
    "for headline in headlines_other:\n",
    "    block=headline.find('h3')\n",
    "    sub_block = block.find('a')\n",
    "    title = sub_block.text\n",
    "    link = sub_block['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        message += title\n",
    "        message += \"\\n\"\n",
    "        message += \"Link: \"\n",
    "        message += link\n",
    "        message += \"\\n\\n\"\n",
    "\n",
    "        print(title)\n",
    "        print(link,end=\"\\n\\n\")\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://fair.org/\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results_pre = soup.find(id=\"ez-home-container-wrap\")\n",
    "\n",
    "message += \"\\nFAIR:\\n\\n\"\n",
    "links_list=[]\n",
    "\n",
    "results=results_pre.find('div',class_=\"widget-wrap\")\n",
    "\n",
    "headlines = results.find_all('h2')\n",
    "\n",
    "for headline in headlines:\n",
    "    block = headline.find('a')\n",
    "    title = block.text\n",
    "    link = block['href']\n",
    "    \n",
    "    if link not in links_list and not is_scraped(link):\n",
    "        message += title\n",
    "        message += \"\\n\"\n",
    "        message += \"Link: \"\n",
    "        message += link\n",
    "        message += \"\\n\\n\"\n",
    "\n",
    "        print(title)\n",
    "        print(link,end=\"\\n\\n\")\n",
    "        \n",
    "        add_link(link)\n",
    "        links_list.append(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smtplib, ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import date \n",
    "\n",
    "port=465\n",
    "context=ssl.create_default_context()\n",
    "\n",
    "sender_email=\"team17finalproject@gmail.com\"\n",
    "receiver_emails=[\"somjadhav787@gmail.com\"]\n",
    "receiver_names = [\"Som\"]\n",
    "\n",
    "today = date.today()\n",
    "today_str = today.strftime(\"%B %d, %Y\")\n",
    "\n",
    "with smtplib.SMTP_SSL(\"smtp.gmail.com\",port,context=context) as server:\n",
    "    server.login(sender_email,\"Abcd123!\")\n",
    "    for x in range(len(receiver_emails)):\n",
    "        mail = MIMEMultipart(\"alternative\")\n",
    "        mail[\"From\"] = \"News Scraper\"\n",
    "        mail[\"To\"] = receiver_names[x]\n",
    "        mail[\"Subject\"] = \"Daily Headlines for \" + receiver_names[x] + \" - \" + today_str\n",
    "        mail.attach(MIMEText(message,\"plain\"))\n",
    "        receiver_email=receiver_emails[x]\n",
    "        server.sendmail(sender_email,receiver_email,mail.as_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import smtplib, ssl\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import date\n",
    "\n",
    "url = \"https://nber.org/new.html#latest\"\n",
    "page=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "results = soup.find('ul')\n",
    "\n",
    "message_nber = \"\\nThis week's NBER working papers:\\n\\n\"\n",
    "\n",
    "headlines = results.find_all('li',class_=\"multiline-li\")\n",
    "\n",
    "for headline in headlines:\n",
    "    block = headline.find('a')\n",
    "    title = block.text\n",
    "    link = block['href']\n",
    "    \n",
    "    message_nber += title\n",
    "    message_nber += \"\\n\"\n",
    "    message_nber += \"Link: \"\n",
    "    message_nber += link\n",
    "    message_nber += \"\\n\\n\"\n",
    "    \n",
    "port=465\n",
    "context=ssl.create_default_context()\n",
    "\n",
    "sender_email=\"team17finalproject@gmail.com\"\n",
    "receiver_email=\"somjadhav787@gmail.com\"\n",
    "\n",
    "today = date.today()\n",
    "today_str = today.strftime(\"%B %d, %Y\")\n",
    "\n",
    "\n",
    "if date.today().weekday() == 0:\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\",port,context=context) as server:\n",
    "        server.login(sender_email,\"Abcd123!\")\n",
    "        mail = MIMEMultipart(\"alternative\")\n",
    "        mail[\"From\"] = \"News Scraper\"\n",
    "        mail[\"To\"] = receiver_email\n",
    "        mail[\"Subject\"] = \"New NBER Working Papers for Week of \" + today_str\n",
    "        mail.attach(MIMEText(message_nber,\"plain\"))\n",
    "        server.sendmail(sender_email,receiver_email,mail.as_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
